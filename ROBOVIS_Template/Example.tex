\documentclass[a4paper,twoside]{article}
\usepackage{graphicx}                                     
\usepackage{adjustbox}   
\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.


\begin{document}

\title{Visual-inertial sensor fusion via machine learning}

\author{\authorname{Habib Boloorchi\sup{1}\orcidAuthor{0000-0000-0000-0000}, He Bai\sup{1}\orcidAuthor{0000-0000-0000-0000} and Christopher Crick\sup{2}\orcidAuthor{0000-0000-0000-0000}}
\affiliation{\sup{1}Department of Computer Science, Oklahoma State University, My Street, MyTown, MyCountry}
\affiliation{\sup{2}Department of Computing, Main University, MySecondTown, MyCountry}
\email{\{f\_author, s\_author\}@ips.xyz.edu, t\_author@dc.mu.edu}
}

\keywords{The paper must have at least one keyword. The text must be set to 9-point font size and without the use of bold or italic font style. For more than one keyword, please use a comma as a separator. Keywords must be titlecased.}

\abstract{Robot localization is a well-studied problem, with many competing solutions, from odometry to inertial measurement to SLAM to GPS and other beacon-based approaches. Some methods work only with certain modalities: measurement of wheel odometry is useless for legged or flying locomotion, for instance. Visual landmark detection and dead reckoning are useful to robots with many different structures and means of locomotion, and can be implemented with very low-cost cameras and inertial measurement units (IMUs). However, existing approaches require fairly sophisticated hardware with precise, rapid, real-time control loops in order to fuse the visual inertial odometry data adequately. We present algorithms to better handle the uncertainty which stems from noisy, inconsistent IMU and machine vision data, using a machine learning approach to provide a robot better awareness of its location and the effect of its self-motion on visual cues. This approach yields robust localization performance which generalizes across very different robot platforms using low-cost sensing and computation.}

\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

Data fusion  in Robotics has a huge and wide spectral of fields. The aim of most scientists in this area is helping robots to be more autonomous than before. One of the puzzles in this area is Navigation and localization is its bottleneck because a robot needs to have an estimation its state. In other words, having consciousness on where it is, will be a must for decision on next steps .
In our research, we found flexibility in data fusion for the process of localization baffling.Not only, Concise timing is matter but we also found lack of a Software that help us handle devices that can do localization which does not required expert, cost and energy. This necessity to consistency and having experienced in mechanical and electronic engineers and an expensive device lead us to create a device that can work better with less consistency in data fusion.
The demand to having data fusion stems from the role of Visual-inertial odometry. Visual-inertial odometry needs to gather data from both perception and Inertial Measurement Unit(IMU). This Odometry can estimate Coordinate in 3 Dimensional world,which Opticounters and GPS can only do it in 2 Dimension, when corresponding data is received in a consistent latency.   


The Perceptio part is added to inertial odometry to be more Robust to uncertainty. Vision of localization can handle the situation in a variety of environments such as GPS unfriendly places and inertial can overcome situations such as dark or low textured, which could not let the camera has good perception.
These challenges shows up when we use visual inertial odometry:
\begin{itemize}
\item Combining data that comes from IMU and camera should offer the least latency and be so accurate. and if not algorithms could not work well.
\item Even if we could create a device that works accurately calibration each time we use it in order to have good results would cost so much time.
\item Cost of stable Visual inertial odometry sensor is so high
\item Most of the Visual inertial Odometry applications are not enough user-friendly for calibration and regular use. you need to be an expert in Control to know the terms.
\end{itemize}
In order to have more sense of these deficiencies, we can come up with an example. In Agriculture Departments, we have a big population of plants that we need to know the effect of each variable on plants. These kinds of research need to observe every plant and this can be tested by taking Photo of each plant. Visual inertial odometry can help us to localize a self-announcement mobile vehicle that can pull the trigger of shutter. Now it comes more baffling when we need an expert in control who is also a scientist in agriculture. 
We offered a machine learning approach that can resolve the combining data, expenses, and requirement of an expert.
We get data to do the preprocessing that does not need the information of the camera or cameras for the perception. It can work with every visual-inertial sensors. These sensors can be cheap because the learning procedure is robust to noises.
In our method we does perception using essential matrix then we take advantage of Random Forest Regression method to estimate the Position of the robot.  

 
 
 
\section{\uppercase{Related Works}}
When it comes to reduce the uncertainty, localization has a vital role \cite{cadena2016past}. 
In other words, Odometry is using motion data comes from sensors in order to reduce the uncertainty of robots position in environment\cite{huang2017visual,valencia2018mapping}.This data can come from rotation of wheels, GPS, IMU(Inertial Measurement Units), Cameras.
In large-scale Domains GPS (Global Positioning System) is one of the most popular for localization but it has some draw backs in smaller space. For example it does not have enough accuracy and also there is some places it is not feasible to use it. For instance, under water, as a GPS-denied circumstance, we can realize that we need a robocentric approach for other ways for odometry \cite{saska2017system}.
IMU(Inertial Measurement Units) is one of the tools that can be helpful in approximation of the trajectory \cite{gui2015mems}. However, when we use IMU some problems as a matter of noisy data and motor noises on devices can be appeared. Furthermore, we need to have trajectory in circumstances as an example Mars Rovers specifically we cannot use it alone.\cite{mars-rover-slam}  
Rotary encoders that can estimate wheels rotation with getting pulse from opto-counters is a technique for wheeled robots while this instrument cannot be useful for highly dynamic cases such as bipedal robots or in three dimensional spaces for flying robots.  \cite{rovio15}.
The human inspired visual ego-motion can be counted as a liable approach to extract trajectory\cite{engel2018direct}. This Motion estimation, which apply machine vision, is called Visual Odometry. One of the most application of Visual Odometry is SLAM (Simultaneous Localization and Mapping) \cite{orb-slam1,orb-slam2,forster2017svo,mueggler2017event}.
One of the problems that ORB-SLAM1\cite{orb-slam1} ,as an example of best Visual Odometry shows is drifting stems from lack of having closed loop algorithms.\cite{7817784}. Another, disadvantage that can lead to get lost as a robot is having fast angular movements that can lead to miss of landmarks. Since these landmarks are critical sources to estimate the position of our path, robot can lose its position. Thus, vision need to be aided by IMU. In addition, the integration of IMU can offers some benefits\cite{leutenegger2015keyframe}. accuracy can be named as one of these advantages\cite{rovio17,schneider2018maplab,sun2018robust}. 

We can integrate images and Inertial data via variety of algorithms. Using probability to estimate the better likelihood of location\cite{bowman2017probabilistic} is one of the approaches to estimate the position. However, It is more popular to do this fusion can be done by extended Kalman filter\cite{lynen2013robust}. The advantage of Extended Kalman Filter and Uncented Kalman Filter from Kalman filter is using Jacobian to extract the position of the robot in nonlinear system \cite{julier2004unscented,wan2000unscented}.

 The major difference between UKF and EKF is their ability to estimate in the nonlinear system. In other words EKF is more for weak nonlinearity system while UKF is for high nonlinearity system. \cite{st2004comparison}
 
Bloesch et al \cite{rovio15,rovio17}mars-rover-slam  offers Iterated Extended Kalman Filter in order to robust the ability of estimation of trajectory. Iterations can robust the ability to find the maximum likelihood to estimate the position of a robot by robocentric formula \cite{rovio15,rovio17}. 


To do landmark detection, Bloesch et al.  employed Shi-Tomasi algorithm \cite{shi1993good}. In this method, landmarks is extracted by utilization of three times down sampling and more probable landmarks between candidates will be chosen. ROVIO (Robust Visual Inertial odometry) uses these landmarks and by warping of land marks the movement in perception can be estimated.

Other innovations can conceive the ego-motion \cite{zhou2017unsupervised} using unsupervised learning. To understand the motion. This algorithm uses Convolutional Neural Network to find out the warping. As we have three dimensional views using RGB-d or Stereo cameras we can both interpret angular and linear movements.

Although, There are several datasets that contains ground truth in 6 degree of freedom that shows angles and also position of the a flying robot and a get data from inertial sensors and stereo cameras\cite{euroc},in this article, we want to make sure that we process data real-time and we also want to use our algorithm on flying robots. so we suggested a creation of device that able to get inertial and perception data relatively. we also suggested a creation of anew device to collect a data to have relative image and IMU data and having less delay with getting th data.

 our perception is based on the aforementioned Rovio \cite{rovio17}, and integrate it to IMU data.  Instead of using iterated Extended Kalman Filter, we are going to take advantage of supervised Learning  to create trajectory.
 Although, our method cannot show the result as accurately as Rovio, we offer benefit of having more robustness to latency inconsistency in devices such as the one show in Figure \ref{fig:vio} which has received in a progress without pattern that we see in Figure \ref{fig:incons}. 
 
 In our approach, we create reliable algorithm which can give function better than other approach when we have latency.
 




\begin{figure}[!h]
  \centering
   {\epsfig{file =inconsistent.png, width = 5.5cm}}
  \caption{Data that received by our Visual inertial sensor.}
  \label{fig:incons}
 \end{figure}
 % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 
 
 
 \section{Method}
 In order to have a flexible algorithm which can neglect inconsistency of delivered data, we create an algorithm that receives data in an uncertain environment. Our method divided to four steps:finding landmarks, tracking best matches of sequenced frames, finding robocenteric differentiation in distance, Fusion Data, and learning them. 
 
 \subsection{Finding Landmarks}
 In every dataset, it is extremely hard and complex to use all data. In addition, It is too time consuming to have all the pixels as data for video or sequence images.Thus, we need to extract features. Harris is one of the most popular feature extractors that finds corners \cite{harris1988combined}. We use Shi-Tomasi \cite{shi1993good} which down-sample the region of interest to have more robustness for noisy images\cite{rovio17}.
 Figure \ref{fig:featuret} shows the extracte features that are circumscribed by patches.
 \begin{figure}
 \includegraphics[width=80mm]{feature.jpg}
   \caption{The right image has the features that has rectangle around it and the left is the features which are extracted}
   \label{fig:featuret}
 \end{figure}
 
 
 
 
 
 \subsection{Tracking The Best Matching Features}
 
 Based on the fact that the Features are unique in each image, it is worth thinking about the challenge of finding associated landmarks  in two consecutive frames. In the approach that we pursue, we create a patch around all of points of interest and flatten them to see them as  vectors. Manhattan distance was our choice to find the similar matches. These matches are sorted and the difference between coordinates of best points of interests. 
 To Normalize data, we insert both the difference of coordinates in each row as well as distance in position of tracked features in a data-frame.
 \subsection{Computing Rotation and Transformation }
 We do the Visual Odometry ignoring the presence of Inertial Measurement Unit. in this section we create a Fundamental matrix \cite{luong1996fundamental} based on the tracked points. 
 The fundamental matrix extract the key points that can give us all information about matching point. This process needs Extrinsics and Intrinsics of the camera in order to give us Epipolar Geometry.\cite{hartley2003multiple} 
 In the following equation we can compute Fundamental Matrix:
 \begin{equation}\label{eq1}
 {\mathbf  {x}}'^{{\top }}{\mathbf  {Fx}}=0
 \end{equation} 
 x and x' are the vector of landmarks and F is fundamental matrix. 
 To find out the distance of the landmarks and Epipolar Geometry is not enough. we need to have Essential Matrix to decompose Rotation and transformation based on that. This is the equation that tells us how to compute Essential Matrix\cite{nister2004efficient} :
  \begin{equation}\label{eq2}
  {\displaystyle \mathbf {E} =({\mathbf {K} '})^{\top }\;\mathbf {F} \;\mathbf {K} }
  \end{equation}
  K and K' is intrinsic of each camera. 
  Essential Matrix consist of Rotation and Translation.Singular Value Decomposition can compute rotation and translation our stereo visual sensor. 
  
 
 \subsection{Fusion Data}
 
 To do visual inertial odometry, we add linear acceleration and angular velocity from Inertial Measurement unit. These data should come together at once. However, there is some latency or difference in frequency in receiving data. To overcome this issue, we proposed a blackboard semi design pattern. 
 In other words, we put all data in global variables and those data that should be used for process will be grabbed by an specific function at the same time.Using this approach we will not have any missing value due to  having multi thread callback functions that update the data from stream which can be come from a robot or a Ros-bag.
 
 In other words, as shown in Figure \ref{fig:Flowchart} we have an algorithm that collect data from Ros-bag topic, and extract features and important data from images and IMU data. In the next step these valuable information will be shared with machine learning function.
  
 
 \subsection{Supervised learning approach}
 We defined a laser data  that put on devices as a ground truth  (Target variable). Our plan is to estimate the position of Visual Inertial Odometry Sensor  via learning. As we see in \ref{Error_table} the linear regression can offer less Root Mean Square error.
 
 \begin{figure}[!h]
   \centering
    {\epsfig{file =flowchart.png, width = 5.5cm}}
   \caption{ROS Topics are streams that convey messages. These messages can be images, texts, or data-structures such as Dictionaries. Callback functions will be run in a loop automatically and put messages from topics in global variables. data will be collected by a preprocessor function. Then Processed data will be shared with the machine learning method.}
   \label{fig:Flowchart}
  \end{figure}
  
 
 
 % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{Experimental Result}
The purpose of this research is  finding the coordinate of the robot regardless of the accuracy and consistency of the latency in Visual Inertial Odometry Sensor.
Figure \ref{fig:rasp} shows the captured data received by the device that we have built ourselves (Figure \ref{fig:vio}). As we can see that the latency is not consistent.
We used Robotic Operating System to make sure that Callback functions can work parallel with main function. In addition, we used KNIME in order to create plots and extract errors. The whole algorithm can work in a regular laptop. All codes has been written in Python 2.7. The image processing algorithms are using OpenCV2 libraries. 
 We tried several regression algorithms:
\begin{itemize}
\item Simple Regression Tree (SRT)
\item Gradient Boosted Trees Regression (GBT)
\item Random Forest Regression (RFR)
\item Linear Regression (LR)
\item Polynomial Regression (PR)

\end{itemize}

All of these algorithms resulted in approximately similar error. Root measure square error was our manifest measurement we had tow candidates , Linear Regression and Polynomial Regression. Between these two linear regression had less Mean absolute error.
We tested linear regression with our data and its ground truth for three columns that we can see in Figure \ref{fig:X} - \ref{fig:Z}.
 
\begin{figure}

  \includegraphics[width=85mm]{Rasp.JPG}
  \caption{Rate of receiving data by Visual inertial Odometry Sensor  }
  \label{fig:rasp}
\end{figure} 



\begin{table}[h]


\caption{These Errors are obtained from Different regressions which can define best algorithms for Learning the position}
\label{Error_table}
\begin{center}
\begin{small}


\begin{tabular}{|c||c||c||c||c||c|}
\hline
& SRT & GBT & RFR & LR & PR\\
\hline
\hline
MAE  & 0.085 & 0.076 & 0.073 & 0.071 & 0.072 \\
\hline

MSE  & 0.02 & 0.017 & 0.016 & 0.014 & 0.014 \\

\hline
RMSE  & 0.02 & 0.017 & 0.016 & 0.014 & 0.014 \\
\hline


\end{tabular}

\end{small}
\end{center}

\end{table}




   

Figure \ref{fig:X} - \ref{fig:Z},  we can see variety of estimations compared to ground truth in position, rotation Angular-velocity and linear-velocity. These estimations are obtained by linear Regression algorithm and Dataset for this experiment is gained from EuroC Dataset \cite{euroc}. 


\begin{figure}

  \includegraphics[width=90mm]{X.png}
  \caption{Estimation error of our methods:  difference of Machine learning output compared to ground truth for target X. }
  \label{fig:X}
\end{figure}
\begin{figure}

  \includegraphics[width=90mm]{Y.png}
  \caption{Estimation error of our methods:  difference of Machine learning output compared to ground truth for target Y. }
  \label{fig:Y}
\end{figure}
\begin{figure}

  \includegraphics[width=90mm]{Z.png}
  \caption{Estimation error of our methods:  difference of Machine learning output compared to ground truth for target X. }
  \label{fig:Z}
\end{figure}



\begin{figure}

  \includegraphics[width=90mm]{vio_sensor.jpg}
  \caption{Visual Inertial Odometry Sensor. This sensor has Global Shutter Cameras which take each frame after receiving 10 message from Inertial Measurement Unit by having external trigger. This device is 50 times cheaper than the high precision identical device. and also has the ability to process data since it has a Raspberry pi 3 on it. }
  \label{fig:vio}
\end{figure}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\section{CONCLUSIONS}

One of the biggest challenges for our proposed approach was to create a method that can work with every devices without  a need  for calibration as well as handling cheap and high latency devices. Our results shows that machine learning can manipulate data regardless inconsistency and other unknown  hardship of uncertainty in data collection. In Other words we obtain visual inertial models better than other approaches in a way that it is robust to latency in recieving data from Inertial Measurment Unit(IMU).

In future, We are planning to reduce the error and also use our algorithm on new unknown devices. We will also use it on mobile phones in order to have less dependency on GPS when it comes to be in GPS-free places.
Next step will be using the distance of the landmarks to the robot in order to normalize and have better results from perception module of our algorithm.
\addtolength{\textheight}{-12cm} 

\bibliographystyle{apalike}
{\small
\bibliography{example}}

\end{document}

